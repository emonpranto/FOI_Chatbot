{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "523d345d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "from llama_parse import LlamaParse\n",
    "from dateutil import parser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.document import Document\n",
    "from langchain_core.messages import HumanMessage\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pathlib import Path\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.llms import Ollama\n",
    "from datetime import datetime, timezone\n",
    "from typing import List, Dict\n",
    "from llama_parse import LlamaParse\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.embeddings import BaseEmbedding\n",
    "from pydantic import PrivateAttr\n",
    "from langchain.chains import LLMChain\n",
    "from pathlib import Path\n",
    "from langchain.memory import ConversationKGMemory\n",
    "import pyttsx3\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import openai\n",
    "import tempfile\n",
    "import scipy.io.wavfile as wav\n",
    "import streamlit as st\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9f0af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "my_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "login(token=my_token)\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "sentences = [\n",
    "    \"That is a happy person\",\n",
    "    \"That is a happy dog\",\n",
    "    \"That is a very happy person\",\n",
    "    \"Today is a sunny day\"\n",
    "]\n",
    "embeddings = model.encode(sentences)\n",
    "similarities = model.similarity(embeddings, embeddings)\n",
    "print(similarities.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d12f5c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 384)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48e9ae20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalEmbedding(BaseEmbedding):\n",
    "    _model: SentenceTransformer = PrivateAttr()\n",
    "    _dim: int = PrivateAttr()\n",
    "\n",
    "    def __init__(self, model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\", **kwargs):\n",
    "        super().__init__(**kwargs)  # important for BaseEmbedding init\n",
    "        self._model = SentenceTransformer(model_name)\n",
    "        self._dim = self._model.get_sentence_embedding_dimension()\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        return self._dim\n",
    "\n",
    "    # --- sync methods ---\n",
    "    def _get_text_embedding(self, text: str):\n",
    "        return self._model.encode(text, convert_to_numpy=True).tolist()\n",
    "\n",
    "    def _get_query_embedding(self, query: str):\n",
    "        return self._model.encode(query, convert_to_numpy=True).tolist()\n",
    "\n",
    "    def _get_text_embeddings(self, texts: List[str]):\n",
    "        return [self._get_text_embedding(t) for t in texts]\n",
    "\n",
    "    def _get_query_embeddings(self, queries: List[str]):\n",
    "        return [self._get_query_embedding(q) for q in queries]\n",
    "\n",
    "    # --- async fallbacks ---\n",
    "    async def _aget_text_embedding(self, text: str):\n",
    "        return self._get_text_embedding(text)\n",
    "\n",
    "    async def _aget_query_embedding(self, query: str):\n",
    "        return self._get_query_embedding(query)\n",
    "\n",
    "    async def _aget_text_embeddings(self, texts: List[str]):\n",
    "        return self._get_text_embeddings(texts)\n",
    "\n",
    "    async def _aget_query_embeddings(self, queries: List[str]):\n",
    "        return self._get_query_embeddings(queries)\n",
    "\n",
    "\n",
    "def extract_page_texts_from_documents(documents: List[Document]) -> List[str]:\n",
    "    \"\"\"Extracts page-wise text content from parsed documents.\"\"\"\n",
    "    return [doc.text_resource.text.strip() for doc in documents if doc.text_resource.text]\n",
    "\n",
    "\n",
    "def split_document_page_level(pages: List[str], filename: str) -> List[Dict]:\n",
    "    \"\"\"Creates page-level chunks only (no text/table separation).\"\"\"\n",
    "    final_chunks = []\n",
    "\n",
    "    for page_num, page in enumerate(pages, start=1):\n",
    "        page_text = page.strip()\n",
    "        if not page_text:\n",
    "            continue\n",
    "\n",
    "        final_chunks.append({\n",
    "            \"type\": \"page\",\n",
    "            \"content\": page_text,\n",
    "            \"file_name\": filename,\n",
    "            \"metadata\": {\n",
    "                \"page_num\": page_num,\n",
    "                \"uploaded_at\": datetime.now().isoformat(),\n",
    "            }\n",
    "        })\n",
    "\n",
    "    return final_chunks\n",
    "\n",
    "\n",
    "def doc_parser(file_path: str, filename: str) -> List[Dict]:\n",
    "    \"\"\"Parses a PDF and returns page-level chunks.\"\"\"\n",
    "    if file_path.endswith('.pdf'):\n",
    "        document = LlamaParse(\n",
    "            result_type='markdown',\n",
    "            api_key=os.getenv(\"LLAMA_PARSE_API_KEY\"),\n",
    "            parse_mode=\"parse_page_with_lvm\"\n",
    "        ).load_data(file_path)\n",
    "\n",
    "        page_texts = extract_page_texts_from_documents(document)\n",
    "        chunks = split_document_page_level(page_texts, filename)\n",
    "        return chunks\n",
    "    else:\n",
    "        print(f\"Unsupported file format: {file_path}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "import faiss\n",
    "import pickle\n",
    "\n",
    "def store_in_faiss(chunks, faiss_index_path=\"faiss_storage\"):\n",
    "    os.makedirs(faiss_index_path, exist_ok=True)\n",
    "    embed_model = LocalEmbedding()\n",
    "    embeddings = [embed_model.get_text_embedding(c[\"content\"]) for c in chunks]\n",
    "\n",
    "    dim = embed_model.dim\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(np.array(embeddings).astype(\"float32\"))\n",
    "\n",
    "    # Save FAISS index\n",
    "    faiss.write_index(index, f\"{faiss_index_path}/faiss.index\")\n",
    "\n",
    "    # Save metadata separately\n",
    "    with open(f\"{faiss_index_path}/metadata.pkl\", \"wb\") as f:\n",
    "        pickle.dump(chunks, f)\n",
    "\n",
    "    print(f\" Stored {len(chunks)} chunks in {faiss_index_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44a00bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"E:\\Backup d\\Pranto Coding Files\\RAG using DeepSeek Innovative Skills\\bots\\document_store\\pdfs\\8 The Tragedy of Julius Caesar author William Shakespeare.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cffb967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8 The Tragedy of Julius Caesar author William Shakespeare'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = Path(file_path).stem.split(\"\\\\\")[0]\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5041aad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    chunks = doc_parser(file_path, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36a1722e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fee52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_in_faiss(chunks, faiss_index_path=\"faiss_storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48e84aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_context_block(text):\n",
    "    if not text:\n",
    "        return None\n",
    "    lowered = text.lower()\n",
    "    if \"insufficient information\" in lowered or \"i can't find\" in lowered or \"no data\" in lowered or \"there is no\" in lowered:\n",
    "        return \"\"\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "493a955b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_faiss_index(faiss_index_path=\"faiss_storage\"):\n",
    "    index = faiss.read_index(f\"{faiss_index_path}/faiss.index\")\n",
    "    with open(f\"{faiss_index_path}/metadata.pkl\", \"rb\") as f:\n",
    "        chunks = pickle.load(f)\n",
    "    return index, chunks\n",
    "\n",
    "\n",
    "def get_context_from_faiss(query, top_k=3, faiss_index_path=\"faiss_storage\"):\n",
    "    embed_model = LocalEmbedding()\n",
    "    query_vec = np.array([embed_model.get_text_embedding(query)]).astype(\"float32\")\n",
    "\n",
    "    index, chunks = load_faiss_index(faiss_index_path)\n",
    "    _ , I = index.search(query_vec, top_k)\n",
    "\n",
    "    results = []\n",
    "    for idx in I[0]:\n",
    "        if idx < len(chunks):\n",
    "            c = chunks[idx]\n",
    "            file_info = c[\"metadata\"].get(\"file_name\", \"unknown\")\n",
    "            page_info = c[\"metadata\"].get(\"page_num\", \"N/A\")\n",
    "            results.append(f\"{c['content']}\\n(file: {file_info}, page: {page_info})\")\n",
    "\n",
    "    return \"\\n\\n\".join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c97f8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\B'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\B'\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_6332\\153811374.py:1: SyntaxWarning: invalid escape sequence '\\B'\n",
      "  HISTORY_FILE = \"E:\\Backup d\\Pranto Coding Files\\RAG using DeepSeek Innovative Skills\\FOI\\conversation_history.txt\"\n"
     ]
    }
   ],
   "source": [
    "HISTORY_FILE = \"E:\\Backup d\\Pranto Coding Files\\RAG using DeepSeek Innovative Skills\\FOI\\conversation_history.txt\"\n",
    "def save_to_history(user_query, answer, history_file=HISTORY_FILE):\n",
    "    os.makedirs(os.path.dirname(history_file), exist_ok=True)\n",
    "\n",
    "    with open(history_file, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"User: {user_query}\\n\")\n",
    "        f.write(f\"Assistant: {answer}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de2cca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama3:8b\")\n",
    "kg_memory = ConversationKGMemory(llm=llm, memory_key=\"chat_memory\", input_key=\"input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea161fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationKGMemory(chat_memory=InMemoryChatMessageHistory(messages=[]), input_key='input', kg=<langchain_community.graphs.networkx_graph.NetworkxEntityGraph object at 0x0000018B3C67AFF0>, llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000018B3DF4FDA0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000018B3E2752E0>, root_client=<openai.OpenAI object at 0x0000018B3C55D400>, root_async_client=<openai.AsyncOpenAI object at 0x0000018B3C67AC30>, temperature=1.0, model_kwargs={}, openai_api_key=SecretStr('**********')), memory_key='chat_memory')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef7e3aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c36840e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_with_memory(user_query, doc):\n",
    "    print(\"Generating answer...\")\n",
    "\n",
    "    global kg_memory\n",
    "    \n",
    "    conversation_prompt = PromptTemplate(\n",
    "        input_variables=[\"chat_memory\", \"input\", \"doc\"],\n",
    "        template=\"\"\"You are a smart assistant. Answer the user's query based on the provided context and conversation facts.\n",
    "        The context will be include text, tables. Please answer the question describing breifly and concisely.\n",
    "        Remember you answer must be contain context and relations that user can understand without any ambiguity or query for extra information.\n",
    "        In response must give the info about the page number which can be found in the metadata if only the result comes from those \n",
    "        specific files.\n",
    "\n",
    "Conversation Facts (from memory):\n",
    "{chat_memory}\n",
    "\n",
    "Document Context:\n",
    "{doc}\n",
    "\n",
    "Rules:\n",
    "- If the document doesn't have the answer, reply: Sorry, I can only answer questions related to Julius Caesar.\n",
    "- If the document has the answer, include page_num along with the answer.\n",
    "- Answer the question breilfy.\n",
    "\n",
    "User Query: {input}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    )\n",
    "\n",
    "    conversation_chain = LLMChain(\n",
    "        llm=llm,\n",
    "        prompt=conversation_prompt,\n",
    "        memory=kg_memory,\n",
    "    )\n",
    "\n",
    "    print(\"KG Triples:\", kg_memory.kg.get_triples())\n",
    "    print(\"Chat Memory Messages:\", kg_memory.chat_memory.messages)\n",
    "    print(\"Memory Variables:\", kg_memory.memory_variables)\n",
    "\n",
    "    result = conversation_chain.predict(\n",
    "        input=user_query,\n",
    "        doc=doc,\n",
    "    )\n",
    "\n",
    "    save_to_history(user_query, result)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b1f222c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationKGMemory(chat_memory=InMemoryChatMessageHistory(messages=[]), input_key='input', kg=<langchain_community.graphs.networkx_graph.NetworkxEntityGraph object at 0x0000018B3C67AFF0>, llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000018B3DF4FDA0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000018B3E2752E0>, root_client=<openai.OpenAI object at 0x0000018B3C55D400>, root_async_client=<openai.AsyncOpenAI object at 0x0000018B3C67AC30>, temperature=1.0, model_kwargs={}, openai_api_key=SecretStr('**********')), memory_key='chat_memory')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91bb2699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_and_transcribe(duration=10, samplerate=16000):\n",
    "    print(\" Recording... Speak now!\")\n",
    "    audio = sd.rec(int(duration * samplerate), samplerate=samplerate, channels=1, dtype=np.int16)\n",
    "    sd.wait()\n",
    "    print(\" Recording finished\")\n",
    "\n",
    "    # Save temporary WAV\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as tmpfile:\n",
    "        wav.write(tmpfile.name, samplerate, audio)\n",
    "        audio_path = tmpfile.name\n",
    "\n",
    "    # Transcribe with Whisper (using OpenAI API)\n",
    "    with open(audio_path, \"rb\") as f:\n",
    "        transcript = openai.audio.transcriptions.create(\n",
    "            model=\"whisper-1\",\n",
    "            file=f\n",
    "        )\n",
    "    os.remove(audio_path) \n",
    "    return transcript.text\n",
    "\n",
    "\n",
    "def speak(text):\n",
    "    engine = pyttsx3.init()\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d14f4266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Recording... Speak now!\n",
      " Recording finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'今日もご視聴ありがとうございました'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = record_and_transcribe()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7fb6861",
   "metadata": {},
   "outputs": [],
   "source": [
    "speak(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "18648556",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = input(\"\")\n",
    "if user == \"y\":\n",
    "    text = record_and_transcribe()\n",
    "else:\n",
    "    text = input(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1f6aaab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'who is thomas?'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c6fc952c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer...\n",
      "KG Triples: [('Caesar', 'a group of conspirators led by Brutus', 'was killed by'), ('Caesar', 'noble Roman', 'was a'), ('Caesar', 'group of conspirators', 'was killed by'), ('group of conspirators', 'Brutus', 'led by'), ('Act 2', \"Caesar's ambition\", 'Brutus ponders')]\n",
      "Chat Memory Messages: [HumanMessage(content='who killed caesar?', additional_kwargs={}, response_metadata={}), AIMessage(content='Caesar was killed by a group of conspirators led by Brutus. (page: unknown)', additional_kwargs={}, response_metadata={}), HumanMessage(content='کیا ہوتا ہے ایکٹ ٹو؟', additional_kwargs={}, response_metadata={}), AIMessage(content='Sorry, I can only answer questions related to Julius Caesar.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What happened in Act 2?', additional_kwargs={}, response_metadata={}), AIMessage(content='In Act 2 of \"The Tragedy of Julius Caesar,\" Brutus contemplates Caesar\\'s death, considering it necessary for the greater good.', additional_kwargs={}, response_metadata={}), HumanMessage(content='who is caesar?', additional_kwargs={}, response_metadata={}), AIMessage(content='Caesar was a noble Roman and a supporter, mentioned in the text. (page: unknown)', additional_kwargs={}, response_metadata={}), HumanMessage(content='Who killed Caesar?', additional_kwargs={}, response_metadata={}), AIMessage(content='Caesar was killed by a group of conspirators led by Brutus. (Page number from metadata: 43)', additional_kwargs={}, response_metadata={}), HumanMessage(content='What happens in Act 2?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"In Act 2, Brutus ponders Caesar's ambition and decides that Caesar's death may be necessary for the greater good. Lucius brings him a candle and a letter.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Which characters appears in Sins 1?', additional_kwargs={}, response_metadata={}), AIMessage(content='Caesar, Brutus, Cassius, Casca, Decius, Metellus, Trebonius, Cinna, Antony, Lepidus, Popilius, and others appear in Act III, Scene 1. The information can be found on page 35 of the document.', additional_kwargs={}, response_metadata={})]\n",
      "Memory Variables: ['chat_memory']\n",
      "Sorry, I can only answer questions related to Julius Caesar.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    context = get_context_from_faiss(text, top_k=3, faiss_index_path=\"faiss_storage\")\n",
    "\n",
    "    answer = generate_answer_with_memory(text,context)\n",
    "    \n",
    "    speak(answer)\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0017cb4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8132c4fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76656ab1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de22d97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
